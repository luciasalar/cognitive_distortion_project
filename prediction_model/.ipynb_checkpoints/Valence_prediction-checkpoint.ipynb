{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/lucia/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "import emot\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>userid</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>date_added_x</th>\n",
       "      <th>time_completed_x</th>\n",
       "      <th>question_order_x</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>q50</th>\n",
       "      <th>q51</th>\n",
       "      <th>q52</th>\n",
       "      <th>q53</th>\n",
       "      <th>q54</th>\n",
       "      <th>q55</th>\n",
       "      <th>q56</th>\n",
       "      <th>q57</th>\n",
       "      <th>swl</th>\n",
       "      <th>like_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05a1a1b67e3d7a923f9d0ee5502757c0</td>\n",
       "      <td>i cover my ears and scream at what i see quot ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-04-11 22:41:00</td>\n",
       "      <td>2011-04-11 22:42:44</td>\n",
       "      <td>7\\5\\1\\10\\2\\19\\16\\3\\15\\6\\9\\17\\12\\11\\8\\4\\20\\13\\1...</td>\n",
       "      <td>White-American</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6a4355b020a885262933c999ae8e75de, 7304b068d93b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>06165dddbf8856657d57d27600c4944c</td>\n",
       "      <td>long week in vacation bible school doing missi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-22 11:36:47</td>\n",
       "      <td>2011-06-22 11:39:02</td>\n",
       "      <td>15\\2\\12\\1\\16\\9\\19\\3\\17\\4\\11\\6\\8\\7\\18\\10\\5\\20\\1...</td>\n",
       "      <td>White-American</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3cbcdc9163fc20e50d8d6f8a9a0d956b, 4484f3edaf8a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>07244b3fd4536a8eea069ab02057e220</td>\n",
       "      <td>come tomorrow i will be home quot  and therefo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-04-26 08:21:57</td>\n",
       "      <td>2011-04-26 08:23:39</td>\n",
       "      <td>10\\15\\4\\12\\16\\1\\7\\9\\2\\17\\13\\6\\19\\14\\11\\3\\18\\20...</td>\n",
       "      <td>White-other nationalities</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0fb5071a5dc785b06c7aad5b8f440ffe, 3990241cc719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0724fe854bd455061ba84efecdeff469</td>\n",
       "      <td>it's time i fire up this life i'm livin' in!, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-02-21 00:26:08</td>\n",
       "      <td>2011-02-21 00:27:07</td>\n",
       "      <td>19\\17\\15\\20\\5\\11\\2\\7\\1\\12\\18\\6\\10\\14\\4\\3\\9\\16\\...</td>\n",
       "      <td>White-other nationalities</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3cbcdc9163fc20e50d8d6f8a9a0d956b, 558e3800b855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                            userid  \\\n",
       "1           1             1  05a1a1b67e3d7a923f9d0ee5502757c0   \n",
       "2           2             2  06165dddbf8856657d57d27600c4944c   \n",
       "3           3             3  07244b3fd4536a8eea069ab02057e220   \n",
       "4           4             4  0724fe854bd455061ba84efecdeff469   \n",
       "\n",
       "                                                text  Unnamed: 2  Unnamed: 3  \\\n",
       "1  i cover my ears and scream at what i see quot ...         NaN         NaN   \n",
       "2  long week in vacation bible school doing missi...         NaN         NaN   \n",
       "3  come tomorrow i will be home quot  and therefo...         NaN         NaN   \n",
       "4  it's time i fire up this life i'm livin' in!, ...         NaN         NaN   \n",
       "\n",
       "          date_added_x     time_completed_x  \\\n",
       "1  2011-04-11 22:41:00  2011-04-11 22:42:44   \n",
       "2  2011-06-22 11:36:47  2011-06-22 11:39:02   \n",
       "3  2011-04-26 08:21:57  2011-04-26 08:23:39   \n",
       "4  2011-02-21 00:26:08  2011-02-21 00:27:07   \n",
       "\n",
       "                                    question_order_x  \\\n",
       "1  7\\5\\1\\10\\2\\19\\16\\3\\15\\6\\9\\17\\12\\11\\8\\4\\20\\13\\1...   \n",
       "2  15\\2\\12\\1\\16\\9\\19\\3\\17\\4\\11\\6\\8\\7\\18\\10\\5\\20\\1...   \n",
       "3  10\\15\\4\\12\\16\\1\\7\\9\\2\\17\\13\\6\\19\\14\\11\\3\\18\\20...   \n",
       "4  19\\17\\15\\20\\5\\11\\2\\7\\1\\12\\18\\6\\10\\14\\4\\3\\9\\16\\...   \n",
       "\n",
       "                   ethnicity  \\\n",
       "1             White-American   \n",
       "2             White-American   \n",
       "3  White-other nationalities   \n",
       "4  White-other nationalities   \n",
       "\n",
       "                         ...                         q50 q51  q52  q53  q54  \\\n",
       "1                        ...                           9   2    6    9    6   \n",
       "2                        ...                           9   9    9    9    9   \n",
       "3                        ...                           9   1    9    9    9   \n",
       "4                        ...                           5   3    9    8    5   \n",
       "\n",
       "   q55  q56  q57  swl                                            like_id  \n",
       "1    7    5    7  2.8  6a4355b020a885262933c999ae8e75de, 7304b068d93b...  \n",
       "2    8    7    9  4.0  3cbcdc9163fc20e50d8d6f8a9a0d956b, 4484f3edaf8a...  \n",
       "3    5    3    9  5.8  0fb5071a5dc785b06c7aad5b8f440ffe, 3990241cc719...  \n",
       "4    9    8    3  6.4  3cbcdc9163fc20e50d8d6f8a9a0d956b, 558e3800b855...  \n",
       "\n",
       "[4 rows x 119 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('../data/status_dep_demog_big5_schwartz_swl_like.csv')\n",
    "dataset[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>userid</th>\n",
       "      <th>negative_yn_self</th>\n",
       "      <th>distortion_yn</th>\n",
       "      <th>quote</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>distortion.category</th>\n",
       "      <th>negative_yn.y</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I messed up so bad and said things didn't mean...</td>\n",
       "      <td>1729ac1c5673a08e2d6b6326f5ad5537</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>labeling/all-or-nothing</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt; |I'm gonna live|I'm gonna survive|Don't want...</td>\n",
       "      <td>cb02460576e27f95654a9a0ef548fe5b</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>4</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>||if home's where my heart is then I'm out of ...</td>\n",
       "      <td>cb02460576e27f95654a9a0ef548fe5b</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ah, the joy of seeing movie that nobody ACTUAL...</td>\n",
       "      <td>82e932005d8c94bb93bcb54fa19f421c</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "1  I messed up so bad and said things didn't mean...   \n",
       "2  < |I'm gonna live|I'm gonna survive|Don't want...   \n",
       "3  ||if home's where my heart is then I'm out of ...   \n",
       "4  Ah, the joy of seeing movie that nobody ACTUAL...   \n",
       "\n",
       "                             userid  negative_yn_self  distortion_yn quote  \\\n",
       "1  1729ac1c5673a08e2d6b6326f5ad5537                 1              1   NaN   \n",
       "2  cb02460576e27f95654a9a0ef548fe5b                 1              2     y   \n",
       "3  cb02460576e27f95654a9a0ef548fe5b                 1              2   NaN   \n",
       "4  82e932005d8c94bb93bcb54fa19f421c                 1              2   NaN   \n",
       "\n",
       "   magnitude      distortion.category negative_yn.y  Positive  Negative  \n",
       "1        4.0  labeling/all-or-nothing           yes         3        -2  \n",
       "2        3.0                      NaN           yes         4        -3  \n",
       "3        2.0                      NaN           yes         1        -3  \n",
       "4        1.0                      NaN            no         3        -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the self-labeled file\n",
    "# In[45]:\n",
    "data = pd.read_csv('../data/self_label_distortion2.csv')\n",
    "data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####this function extract emoticons from the text and return a df with emoticons\n",
    "def extract_emoticon(text):\n",
    "    emo_fea = []\n",
    "    for i in text:\n",
    "        emo = emot.emoticons(i)\n",
    "        if len(emo) > 0:\n",
    "            emo_fea.append(emo[0]['value'])\n",
    "        else:\n",
    "            emo_fea.append(False)\n",
    "            emo_df = pd.DataFrame(emo_fea)\n",
    "    return emo_df\n",
    "\n",
    "###process data\n",
    "\n",
    "def preprocess(sent):\n",
    "\n",
    "    words = str(sent).split()\n",
    "    new_words = []\n",
    "    # ps = PorterStemmer()\n",
    "    \n",
    "    for w in words:\n",
    "        w = w.lower().replace(\"**bobsnewline**\",\"\")\n",
    "        # remove non English word characters\n",
    "        w = re.sub(r'[^\\x00-\\x7F]+',' ', w)\n",
    "        # remove puncutation \n",
    "        w = re.sub(r'[^\\w\\s]','',w)\n",
    "        # w = ps.stem(w)\n",
    "        new_words.append(w)\n",
    "        \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "emo_df = extract_emoticon(data['text'])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add emoticon column to the feature df\n",
    "frames = [data, emo_df]\n",
    "data1 = pd.concat(frames, axis = 1)\n",
    "data1.columns.values[10] = 'emoticon'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 130)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove na in id\n",
    "data2 = data1[pd.notnull(data1['userid'])]\n",
    "#senti strength score\n",
    "data2['senti_score'] = data2['Positive'] + data2['Negative']\n",
    "\n",
    "###merge text, id with all FB features\n",
    "all_data = pd.merge(data2, dataset, on = 'userid', how = 'inner')\n",
    "all_data[1:5]\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####convert categorical data \n",
    "#select useful variables in dataset\n",
    "selected = ['userid', 'marital_status', 'ethnicity', 'gender','age','relationship_status', 'network_size','negative_yn_self', 'negative_yn','text_x','Positive','Negative','senti_score','distortion_yn', 'magnitude']\n",
    "data_n = all_data.loc[:, (selected)]\n",
    "data_dep = data_n\n",
    "data_dep = data_dep[pd.notnull(data_dep['text_x'])]\n",
    "\n",
    "#fill na with mean\n",
    "data_dep['ethnicity'] = data_dep['ethnicity'].fillna('Other')\n",
    "data_dep['marital_status'] = data_dep['marital_status'].fillna('Other')\n",
    "data_dep['age'] = data_dep['age'].fillna(data_dep['age'].mean())\n",
    "data_dep['relationship_status'] = data_dep['relationship_status'].fillna(0)\n",
    "data_dep['network_size'] = data_dep['network_size'].fillna(data_dep['network_size'].mean())\n",
    "\n",
    "####one hot encoding\n",
    "features_oneHot = ['marital_status', 'ethnicity', 'gender','relationship_status']\n",
    "\n",
    "x = data_dep[features_oneHot].values\n",
    "y = data_dep['negative_yn_self'].values\n",
    "# Encoding categorical data\n",
    "\n",
    "marital_status = pd.get_dummies(x[:, 0])\n",
    "ethnicity = pd.get_dummies(x[:, 1])\n",
    "gender = pd.get_dummies(x[:, 2])\n",
    "relationship_status = pd.get_dummies(x[:, 3])\n",
    "#emoticon = pd.get_dummies(x[:, 4])\n",
    "\n",
    "fea = pd.concat([marital_status, ethnicity, gender,relationship_status], axis =1).values\n",
    "fea[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add one hot feature with other features\n",
    "features = ['age','network_size','Positive','Negative','senti_score'] \n",
    "x2 = data_dep[features].values\n",
    "\n",
    "fb_fea = np.concatenate((fea, x2), axis=1)\n",
    "##add LIWC\n",
    "text_id = data_dep[['text_x','userid']]\n",
    "\n",
    "###this is the text to be processed by LIWC  \n",
    "data_dep['text_x'].to_csv('../data/liwc.csv')\n",
    "# select LIWC data, scale it to remove 0\n",
    "text_liwc = pd.read_csv('../data/distortion_liwc_1000.csv')\n",
    "liwc = text_liwc.loc[:,'function':'OtherP'].values\n",
    "#Center to the mean and component wise scale to unit variance.\n",
    "#liwc_scaled = preprocessing.scale(liwc)\n",
    "#liwc_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#here we use word vector as features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###word embedding text as feature\n",
    "cv = CountVectorizer()\n",
    "text_vec = cv.fit_transform(text_liwc['text']).toarray()\n",
    "\n",
    "####convert to tfidf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "text_vec = tfidf_transformer.fit_transform(text_vec).toarray()\n",
    "\n",
    "####combine with all features\n",
    "X = text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize data with standardscaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6995614035087719\n",
      "{'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.336623 (0.001781) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.683114 (0.038260) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.666667 (0.004478) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.336623 (0.001781) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.337719 (0.002190) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.336623 (0.001781) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.336623 (0.001781) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.336623 (0.001781) with: {'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.533991 (0.159812) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.649123 (0.019558) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.546053 (0.127589) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.669956 (0.028118) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.402412 (0.131042) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.361842 (0.013919) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.533991 (0.159812) with: {'svc__C': 0.3, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.533991 (0.159812) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.657895 (0.026846) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.615132 (0.005882) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.671053 (0.032707) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.402412 (0.131042) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.379386 (0.016558) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.533991 (0.159812) with: {'svc__C': 0.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.663377 (0.001781) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.653509 (0.018100) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.591009 (0.015483) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.656798 (0.030022) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.402412 (0.131042) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.399123 (0.021691) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.605263 (0.068224) with: {'svc__C': 0.7, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.663377 (0.001781) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.628289 (0.030938) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.605263 (0.017164) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.652412 (0.028535) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.401316 (0.131621) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.421053 (0.025746) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.698465 (0.015322) with: {'svc__C': 0.9, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.663377 (0.001781) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.645833 (0.016270) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.614035 (0.012137) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.658991 (0.031474) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.473684 (0.156032) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.430921 (0.023215) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.696272 (0.014164) with: {'svc__C': 1.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.663377 (0.001781) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.631579 (0.015560) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.665570 (0.004984) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.629386 (0.010343) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.657895 (0.035524) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.663377 (0.001781) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.588816 (0.032747) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.699561 (0.030393) with: {'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.005592) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'poly'}\n",
      "0.663377 (0.001781) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'rbf'}\n",
      "0.618421 (0.033103) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'linear'}\n",
      "0.664474 (0.006262) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'poly'}\n",
      "0.631579 (0.010845) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'rbf'}\n",
      "0.664474 (0.034282) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.001, 'svc__kernel': 'sigmoid'}\n",
      "0.694079 (0.023553) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'linear'}\n",
      "0.663377 (0.001781) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'poly'}\n",
      "0.677632 (0.032563) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'rbf'}\n",
      "0.687500 (0.023202) with: {'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "#####grid search (the parameters predict everything to one class, we should use a separated \n",
    "#sample for tuning parameters, but not enough cases so far)\n",
    "##0.6995614035087719  best result\n",
    "#{'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "svc = make_pipeline(StandardScaler(),svm.SVC())\n",
    "parameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0,] , 'svc__class_weight':['balanced']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(estimator = svc,\n",
    "                          param_grid = parameters,\n",
    "                           cv =  cv,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66478799 0.59938767 0.68967462 0.68940601 0.67767971]\n",
      "f1: 0.66 (+/- 0.07)\n",
      "0.4952841882744218\n",
      "0.6058170280274987\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(),svm.SVC(gamma=0.001, class_weight='balanced', C = 1.5, kernel = 'sigmoid'))\n",
    "scores = cross_val_score(svc, X, y, cv=cv, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "precision = cross_val_score(svc, X, y, cv=cv, scoring='precision')\n",
    "recall = cross_val_score(svc, X, y, cv=cv, scoring='recall')\n",
    "print(precision.mean())\n",
    "print(rec8all.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6644736842105263\n",
      "{'sgdclassifier__alpha': 0.005, 'sgdclassifier__class_weight': 'balanced', 'sgdclassifier__loss': 'modified_huber', 'sgdclassifier__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#####grid search (the parameters predict everything to one class, we should use a separated \n",
    "#sample for tuning parameters, but not enough cases so far)\n",
    "clf = make_pipeline(StandardScaler(),SGDClassifier(max_iter= 1000))\n",
    "#clf = SGDClassifier(max_iter= 1000)\n",
    "parameters = [{'sgdclassifier__alpha': [0.01, 0.05, 0.001, 0.005], 'sgdclassifier__class_weight':['balanced'],\n",
    "              'sgdclassifier__loss': ['hinge','log','modified_huber','squared_hinge', 'perceptron'], \n",
    "               'sgdclassifier__penalty':['none','l1','l2']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(clf,\n",
    "                          param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = cv,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X, y)\n",
    "\n",
    "print(grid_search.best_score_)####not sure how come this is so good,but the parameters don't work good in the model\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#text feature: wordvec + LIWC (no feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####combine with liwc\n",
    "X_liwc = np.concatenate((X, liwc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7302631578947368\n",
      "{'svc__C': 2.0, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "#####grid search (the parameters predict everything to one class, we should use a separated \n",
    "#sample for tuning parameters, but not enough cases so far)\n",
    "##0.6995614035087719  best result\n",
    "#{'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n",
    "cv = StratifiedKFold(n_splits=5, random_state = 0)\n",
    "svc = make_pipeline(StandardScaler(),svm.SVC())\n",
    "parameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0,] , 'svc__class_weight':['balanced']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(estimator = svc,\n",
    "                          param_grid = parameters,\n",
    "                           cv =  cv,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X_liwc, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74747119 0.66712111 0.74568893 0.68807222 0.74376738]\n",
      "f1: 0.72 (+/- 0.07)\n",
      "0.6400892074459856\n",
      "0.4692226335272342\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(),svm.SVC(gamma=0.0001, class_weight='balanced', C = 2, kernel = 'sigmoid'))\n",
    "scores = cross_val_score(svc, X_liwc, y, cv=cv, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "precision = cross_val_score(svc, X_liwc, y, cv=cv, scoring='precision')\n",
    "recall = cross_val_score(svc, X_liwc, y, cv=cv, scoring='recall')\n",
    "print(precision.mean())\n",
    "print(recall.mean())  #recall too low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#text feature: wordvec + LIWC + demo 'marital_status', 'ethnicity', 'gender','relationship_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####combine with liwc\n",
    "X_demog = np.concatenate((X_liwc, fea), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 5517)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_demog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7247807017543859\n",
      "{'svc__C': 1.5, 'svc__class_weight': 'balanced', 'svc__gamma': 0.0001, 'svc__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "#####grid search (the parameters predict everything to one class, we should use a separated \n",
    "#sample for tuning parameters, but not enough cases so far)\n",
    "svc = make_pipeline(StandardScaler(),svm.SVC())\n",
    "parameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0,] , 'svc__class_weight':['balanced']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(estimator = svc,\n",
    "                          param_grid = parameters,\n",
    "                           cv =  cv,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X_demog, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73884808 0.65345851 0.73294114 0.72003085 0.72047351]\n",
      "f1: 0.71 (+/- 0.06)\n",
      "0.6252197749295839\n",
      "0.4660497091485986\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(),svm.SVC(gamma=0.0001, class_weight='balanced', C = 1.5, kernel = 'sigmoid'))\n",
    "scores = cross_val_score(svc, X_demog, y, cv=cv, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "precision = cross_val_score(svc, X_demog, y, cv=cv, scoring='precision')\n",
    "recall = cross_val_score(svc, X_demog, y, cv=cv, scoring='recall')\n",
    "print(precision.mean())\n",
    "print(recall.mean())  #recall too low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7214912280701754\n",
      "{'svc__C': 0.1, 'svc__class_weight': 'balanced', 'svc__gamma': 0.01, 'svc__kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SelectFromModel(LinearSVC(C=1.5, penalty=\"l2\", dual=False)), svm.SVC(gamma=0.0001, class_weight='balanced', C = 1.5, kernel = 'sigmoid'))\n",
    "parameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0,] , 'svc__class_weight':['balanced']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(estimator = svc,\n",
    "                          param_grid = parameters,\n",
    "                           cv =  cv,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X_demog, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73862186 0.6629387  0.74119275 0.7429567  0.74914068]\n",
      "f1: 0.73 (+/- 0.06)\n",
      "0.5716852063884027\n",
      "0.7004230565838181\n"
     ]
    }
   ],
   "source": [
    "svc = make_pipeline(StandardScaler(), SelectFromModel(LinearSVC(C=1.5, penalty=\"l2\", dual=False)), svm.SVC(gamma=0.01, class_weight='balanced', C = 0.1, kernel = 'sigmoid'))\n",
    "scores = cross_val_score(svc, X_demog, y, cv=cv, scoring='f1_weighted')\n",
    "print(scores)\n",
    "print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "precision = cross_val_score(svc, X_demog, y, cv=cv, scoring='precision')\n",
    "recall = cross_val_score(svc, X_demog, y, cv=cv, scoring='recall')\n",
    "print(precision.mean())\n",
    "print(recall.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Wordvec + LIWC fea select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = make_pipeline(StandardScaler(), SelectFromModel(LinearSVC(C=1.5, penalty=\"l2\", dual=False)), svm.SVC(gamma=0.0001, class_weight='balanced', C = 1.5, kernel = 'sigmoid'))\n",
    "parameters = [{'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'svc__gamma': [0.01, 0.001, 0.0001],\n",
    "                     'svc__C':[0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.5, 2.0,] , 'svc__class_weight':['balanced']}]\n",
    "                   \n",
    "grid_search_item = GridSearchCV(estimator = svc,\n",
    "                          param_grid = parameters,\n",
    "                           cv =  cv,\n",
    "                           scoring = 'accuracy',\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search_item.fit(X_demog, y)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 5455)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_liwc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##fast text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fastText.FastText in fastText:\n",
      "\n",
      "NAME\n",
      "    fastText.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the BSD-style license found in the\n",
      "    # LICENSE file in the root directory of this source tree. An additional grant\n",
      "    # of patent rights can be found in the PATENTS file in the same directory.\n",
      "\n",
      "FUNCTIONS\n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=3, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(input, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5, minCount=5, minCountLabel=0, minn=3, maxn=6, neg=5, wordNgrams=1, loss='ns', bucket=2000000, thread=3, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /Users/lucia/anaconda3/lib/python3.6/site-packages/fasttext-0.8.22-py3.6-macosx-10.7-x86_64.egg/fastText/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fastText.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
