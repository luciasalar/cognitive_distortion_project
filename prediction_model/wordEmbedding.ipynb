{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('../data/self_label_distortion2.csv')\n",
    "data.columns\n",
    "print('loaded data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyFea:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.userid = hash(self.text)\n",
    "        self.label = []\n",
    "        self.vectors = []\n",
    "        self.meanVec = []\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return self.quoteID\n",
    "    \n",
    "objects = {}\n",
    "with open('../data/self_label_distortion2.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        #print(row['text'])\n",
    "        texthash = hash(row['text'])\n",
    "        if texthash not in objects:\n",
    "            objects[texthash] = MyFea(row['text'])\n",
    "        objects[texthash].label.append(row['negative_yn_self'])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange marriage means ppl walking and unfortunately snake bites... and love marriage means ppl go to the snake say... KAAT LE KAAT LE....!\n",
      "I messed up so bad and said things didn't mean to say :( what can i do to fix everything. All my life break things and try to put them back together, don't wanna break anything anymore :'( baby love you Diana Campero Vigueras <\n",
      "< |I'm gonna live|I'm gonna survive|Don't want the world to pass me by|I'm gonna dream|I aint gonna die|Thinking my life was just lie|I wanna be loved| <\n",
      "||if home's where my heart is then I'm out of place| Lord, won't You give me strength to make it through somehow| I've never been more homesick than now| help me Lord, cuz don't understand Your ways...|| <\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in objects:\n",
    "    print (objects[item].vectors)\n",
    "    count += 1\n",
    "    if count > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('start to load language model..')\n",
    "#sentivec \n",
    "en_model = KeyedVectors.load_word2vec_format('/afs/inf.ed.ac.uk/user/s16/s1690903/share/fasttext/wikipedia.300d.txt')\n",
    "print('Finish loading model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am doing experiment'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(sent):\n",
    "    #remove punctustion\\n\",\n",
    "    sent = re.sub(r'[^\\w\\s]','',sent)\n",
    "    words = sent.split()\n",
    "    new_words = []\n",
    "    for w in words:     \n",
    "        new_words.append(w.lower())\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "sent = \"I am doing Experiment!!!\"\n",
    "preprocess(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_vec(objects):\n",
    "    for item in objects: \n",
    "        text = preprocess(objects[item].text)\n",
    "        words = filter(lambda x: x in en_model.vocab, text.split())\n",
    "        #objects[item].vectors.append([en_model[x] for x in words])\n",
    "        aver = np.mean(np.array([en_model[x] for x in words]))\n",
    "        objects[item].meanVec.append(aver)\n",
    "\n",
    "convert_vec(objects)\n",
    "print('converted text to vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for item in objects:\n",
    "    print (objects[item].meanVec)\n",
    "#     for i in objects[item].meanVec:\n",
    "# #         flat_list.append(i)\n",
    "#         print(i)\n",
    "    count += 1\n",
    "    if count > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump object to pickles\n",
    "filename = 'sentiVectors2'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(objects,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('../data/self_label_distortion2.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text.append(\"\". join(row['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = open('../data/self_label_distortion2.csv').read()\n",
    "text = []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    " #   labels.append(content[3])\n",
    "    text.append(\"\".join(content[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text,userid,negative_yn_self,distortion_yn,quote,magnitude,distortion.category,negative_yn.y,Positive,Negative',\n",
       " 'arrange',\n",
       " '\"I',\n",
       " '<',\n",
       " '\"||if',\n",
       " '\"Ah,',\n",
       " '\"Been',\n",
       " '\"Big-Boy',\n",
       " '\"did',\n",
       " 'Got',\n",
       " 'hates',\n",
       " '\"How',\n",
       " '\"I',\n",
       " '\"I\\'m',\n",
       " '\"If',\n",
       " '\"is',\n",
       " 'NO',\n",
       " '\"The',\n",
       " '\"Unfortunately',\n",
       " '\":',\n",
       " '\":',\n",
       " ':',\n",
       " '*playing',\n",
       " '//',\n",
       " 'Buddy',\n",
       " 'DAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYYYYYYYYYYYYYYYYYYYYYYZZZZZZZZZZZZ!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!,d67916bbf6453cfebbd8689f7556e526,2,2,,NA,,mixed,3,-1',\n",
       " '\"Dear',\n",
       " '\"fed',\n",
       " 'Feels',\n",
       " 'Had',\n",
       " '\"had',\n",
       " '\"Hm\\'k,',\n",
       " '\"Huge',\n",
       " '\"I',\n",
       " 'I',\n",
       " 'i',\n",
       " '\"im',\n",
       " 'in',\n",
       " '\"Is',\n",
       " '\"is',\n",
       " \"isn't\",\n",
       " 'jobs',\n",
       " '\"Just',\n",
       " 'Just',\n",
       " 'Looking',\n",
       " '\"Looking',\n",
       " '\"mini',\n",
       " 'My',\n",
       " '\"okay,',\n",
       " '\"pretty',\n",
       " 'Sam',\n",
       " '\"Sammy\\'s',\n",
       " '\"Things',\n",
       " '\"This\\'ll',\n",
       " '\"Trust',\n",
       " 'trust',\n",
       " 'Two',\n",
       " '\"Weeded',\n",
       " 'Well',\n",
       " 'If',\n",
       " '\"At',\n",
       " '\"If',\n",
       " '\"How',\n",
       " '\"You',\n",
       " '\"stupid',\n",
       " '\"I',\n",
       " 'Feels',\n",
       " '\"i',\n",
       " 'last',\n",
       " 'wishes',\n",
       " 'is',\n",
       " 'Is',\n",
       " 'I',\n",
       " '\"I',\n",
       " '\"My',\n",
       " 'OMG!',\n",
       " \"What's\",\n",
       " 'Got',\n",
       " 'Just',\n",
       " 'I',\n",
       " '\"People',\n",
       " 'I',\n",
       " '\"is',\n",
       " 'Inside',\n",
       " 'Back',\n",
       " '\"Fencing',\n",
       " '\"Going',\n",
       " 'Most',\n",
       " '\"My',\n",
       " '\"So,',\n",
       " '\"This',\n",
       " '\"Why',\n",
       " '\"WOOT!',\n",
       " '\"Life',\n",
       " 'needs',\n",
       " 'The',\n",
       " '\"/And',\n",
       " 'Anyone',\n",
       " '\"Got',\n",
       " '\"I',\n",
       " '\"is',\n",
       " '\"I',\n",
       " '\"I',\n",
       " 'I',\n",
       " \"I've\",\n",
       " '\"Life',\n",
       " '\"Sometimes',\n",
       " 'How',\n",
       " '\"/.../\"\"',\n",
       " '\"I',\n",
       " '\"DANGER:',\n",
       " '\"I',\n",
       " \"I've\",\n",
       " '\"You',\n",
       " '\"Everyday',\n",
       " '\"is',\n",
       " '\"wonders',\n",
       " '\"?\"\"',\n",
       " '\"Friends',\n",
       " '\"had',\n",
       " 'I',\n",
       " '\"Is',\n",
       " 'roast',\n",
       " '\"thanks',\n",
       " 'to',\n",
       " \"Economist's\",\n",
       " 'I',\n",
       " '\"There',\n",
       " '\"Civil',\n",
       " '\"Hello,',\n",
       " '\"loves',\n",
       " 'Finally',\n",
       " 'Probably',\n",
       " \"tis'\",\n",
       " '\"But',\n",
       " '\"So',\n",
       " 'What',\n",
       " '\"Her',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"So',\n",
       " '\"You\\'re',\n",
       " 'i',\n",
       " 'tomorrow',\n",
       " '\"paintballing',\n",
       " '\"Since',\n",
       " 'a',\n",
       " 'Did',\n",
       " '\"Some',\n",
       " 'Birdhouse',\n",
       " '\"Eating',\n",
       " '\"Change',\n",
       " '\"i',\n",
       " 'Creepy',\n",
       " '\"Jeepers...',\n",
       " 'My',\n",
       " 'camping',\n",
       " '\"I',\n",
       " 'Just',\n",
       " 'My',\n",
       " '\"Right...',\n",
       " '\"RT',\n",
       " '\"So',\n",
       " '\"Watermelon...',\n",
       " '\"You\\'ve',\n",
       " '\"Arg,',\n",
       " '\"I',\n",
       " '\"a',\n",
       " 'at',\n",
       " '\"ALL',\n",
       " 'This',\n",
       " '-.-',\n",
       " '\"GAAAAHHH!!!!',\n",
       " 'XO',\n",
       " 'Oh',\n",
       " 'so',\n",
       " 'Part',\n",
       " '\"Pocket',\n",
       " '\"The',\n",
       " '\"Alright.',\n",
       " 'Anyone',\n",
       " '\"So.',\n",
       " '\"The',\n",
       " '\"?\"\"Smile',\n",
       " '\"just',\n",
       " '\"ok',\n",
       " '\"-->[PHONE]<--',\n",
       " '\"*...you',\n",
       " '\"Dear',\n",
       " '\"Homework',\n",
       " 'just',\n",
       " '\"So',\n",
       " 'So',\n",
       " 'Two',\n",
       " '\"But',\n",
       " 'Chemistry',\n",
       " '\"Church,',\n",
       " '\"Employers',\n",
       " '\"Every',\n",
       " '\"First',\n",
       " '\"How',\n",
       " 'I',\n",
       " '\"I',\n",
       " '\"I\\'ll',\n",
       " 'In',\n",
       " 'is',\n",
       " 'is',\n",
       " \"It's\",\n",
       " '\"Lab',\n",
       " 'Last',\n",
       " '\"My',\n",
       " '\"So',\n",
       " '\"Some',\n",
       " '\"Up',\n",
       " 'What',\n",
       " \"i'm\",\n",
       " 'poor',\n",
       " 'Did',\n",
       " '\"I',\n",
       " '\"I\\'m',\n",
       " '\"Doctor',\n",
       " '\"I',\n",
       " 'loves',\n",
       " '\"When',\n",
       " '\"You',\n",
       " '\"Just',\n",
       " 'Beth-Elohim',\n",
       " '\"okay',\n",
       " 'Maybe',\n",
       " '\"::',\n",
       " 'is',\n",
       " '\"||Don?\\'t',\n",
       " '\"I',\n",
       " '\"It',\n",
       " '\"You',\n",
       " '\"...during',\n",
       " '\"And',\n",
       " 'is',\n",
       " '\"It',\n",
       " '\"Well,',\n",
       " '\"What',\n",
       " 'when',\n",
       " '\"I',\n",
       " '\"I',\n",
       " 'Oh',\n",
       " 'Happy',\n",
       " '\"I',\n",
       " 'So.',\n",
       " \"I'm\",\n",
       " '\"i',\n",
       " '\"Police',\n",
       " '\"can\\'t',\n",
       " \"i'm\",\n",
       " 'good',\n",
       " 'Think',\n",
       " 'WOW!!',\n",
       " '\"Beat',\n",
       " '\"I',\n",
       " '\"Star',\n",
       " 'Is',\n",
       " '\"Just',\n",
       " 'My',\n",
       " 'So',\n",
       " 'woke',\n",
       " '\"Delicious',\n",
       " '\"Feels',\n",
       " 'Had',\n",
       " 'Happy',\n",
       " 'Has',\n",
       " 'is',\n",
       " 'Just',\n",
       " '\"Mummy',\n",
       " 'Shrek',\n",
       " 'Will',\n",
       " \"It's\",\n",
       " 'Finished',\n",
       " 'Getting',\n",
       " '\"Had',\n",
       " 'Just',\n",
       " '\"look',\n",
       " 'The',\n",
       " 'god',\n",
       " '\"Robin:',\n",
       " '\"Sad',\n",
       " '\"I',\n",
       " 'Took',\n",
       " 'Wondering',\n",
       " 'Looks',\n",
       " 'Third',\n",
       " '\"Say',\n",
       " '\"Back-In-',\n",
       " 'Geh....woke',\n",
       " 'Why',\n",
       " 'Go',\n",
       " 'Would',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"Lol,',\n",
       " 'love',\n",
       " '\"today',\n",
       " '\"...ok,',\n",
       " '\"is',\n",
       " 'the',\n",
       " '\"Delegates',\n",
       " 'So',\n",
       " 'I',\n",
       " '\"I',\n",
       " '\"Ok,',\n",
       " '\"I\\'m',\n",
       " '\"Salve!',\n",
       " 'We',\n",
       " 'Half',\n",
       " '\"Are',\n",
       " 'COPY',\n",
       " '\"The',\n",
       " 'Wishing',\n",
       " \"What's\",\n",
       " '\"Yesterday,',\n",
       " '\"I',\n",
       " 'So',\n",
       " 'Im',\n",
       " 'so',\n",
       " '\"it`s',\n",
       " '\"sub',\n",
       " 'Finally',\n",
       " 'I',\n",
       " '\"If',\n",
       " 'so',\n",
       " 'What',\n",
       " 'everyone',\n",
       " '\"/Kill',\n",
       " '\"/What',\n",
       " 'Did',\n",
       " 'has',\n",
       " 'Matt:',\n",
       " 'how',\n",
       " 'My',\n",
       " 'Before',\n",
       " '\"i',\n",
       " '\"Rather',\n",
       " 'I',\n",
       " '\"I\\'M',\n",
       " 'BEING',\n",
       " \"CAN'T\",\n",
       " '\"FOR',\n",
       " 'JUST',\n",
       " 'OMG',\n",
       " 'Does',\n",
       " 'I',\n",
       " 'why',\n",
       " 'Remember',\n",
       " '\"post-concussion,',\n",
       " '\"To',\n",
       " '\"(Reposted',\n",
       " 'Had',\n",
       " 'Winter',\n",
       " '\"would',\n",
       " '\"Is',\n",
       " '\"Thanks',\n",
       " '\"PROMISE',\n",
       " '\"There\\'s',\n",
       " '\"Heading',\n",
       " '\"the',\n",
       " '\"Generation',\n",
       " '\"I',\n",
       " '\"Finally',\n",
       " '\"She\\'s',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"If',\n",
       " 'Newton',\n",
       " 'Stressed',\n",
       " '\"Best',\n",
       " '\"can',\n",
       " '\"Dad',\n",
       " '\"finds',\n",
       " 'I',\n",
       " '\"I',\n",
       " '\"Still',\n",
       " '\"Butterflies',\n",
       " '\"Italy',\n",
       " 'Team',\n",
       " 'wishes',\n",
       " '\"/Blackbird',\n",
       " 'incense...zen',\n",
       " '\"just',\n",
       " 'me',\n",
       " '\"reeeeeeaaaaaaaaally',\n",
       " 'Would',\n",
       " '\"More',\n",
       " '\"Ok,',\n",
       " 'Argh!',\n",
       " '\"don\\'t',\n",
       " '\"Errr.',\n",
       " 'I',\n",
       " 'I',\n",
       " '\"Code',\n",
       " '\"Don\\'t',\n",
       " '\"I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'I',\n",
       " 'Like',\n",
       " '\"Should',\n",
       " '\"The',\n",
       " '\"last',\n",
       " '\"/There',\n",
       " '\"Silence',\n",
       " 'The',\n",
       " '\"Today\\'s',\n",
       " 'Your',\n",
       " '\"I',\n",
       " 'Suuuuuuushi!',\n",
       " 'Want',\n",
       " 'Starting',\n",
       " '\"/Sin',\n",
       " '\"Has',\n",
       " 'You',\n",
       " '\"Also...',\n",
       " '\"Blah',\n",
       " '\"Bored',\n",
       " '\"Had',\n",
       " 'Learned',\n",
       " '\"Of',\n",
       " '\"Planning',\n",
       " 'Playstation',\n",
       " 'So',\n",
       " \"Ughhhhhhhhhhhh...Haven't\",\n",
       " 'Wow.....I',\n",
       " '\"Excited',\n",
       " '\"Finished',\n",
       " '\"Got',\n",
       " '\"Had',\n",
       " '\"Halloween',\n",
       " '\"Happy',\n",
       " 'If',\n",
       " '\"Is',\n",
       " 'Is',\n",
       " 'Just',\n",
       " 'Lost',\n",
       " '\"PLUR-B-Q',\n",
       " '\"So',\n",
       " '\"So.....I',\n",
       " '\"Whoaaaaaa',\n",
       " '\"Wow',\n",
       " 'XD',\n",
       " 'You',\n",
       " '\"I',\n",
       " 'Bonnie',\n",
       " 'Happy',\n",
       " 'OMG',\n",
       " '\"cannot',\n",
       " '\"/The',\n",
       " '\"finished',\n",
       " '\"It',\n",
       " '\"do',\n",
       " 'Q:',\n",
       " 'Hey',\n",
       " '\"I\\'m',\n",
       " 'Just',\n",
       " '\"She',\n",
       " '\"So,',\n",
       " '\"*Reality',\n",
       " '\"All',\n",
       " '\"And',\n",
       " 'Can',\n",
       " '\"How',\n",
       " '\"I\\'m',\n",
       " 'Looking',\n",
       " '\"Finally',\n",
       " '\"I',\n",
       " 'Inside',\n",
       " '\"just',\n",
       " '\"Your',\n",
       " '\"To',\n",
       " '\"Well',\n",
       " \"I'm\",\n",
       " '\"Just',\n",
       " '\"there',\n",
       " '\"Watching',\n",
       " 'First',\n",
       " 'Today',\n",
       " '\"Can\\'t...hold...on...much...looooongerrrrrrrr!',\n",
       " 'Class',\n",
       " '\"Unless',\n",
       " '\"Ever',\n",
       " '\"Going',\n",
       " '\"Pressure,',\n",
       " 'Sees',\n",
       " '\"We',\n",
       " 'Attention',\n",
       " 'has',\n",
       " 'is',\n",
       " 'is',\n",
       " 'three',\n",
       " 'wonders',\n",
       " '\"Is',\n",
       " 'i',\n",
       " '\"To',\n",
       " '\"...',\n",
       " '\"Hi,',\n",
       " 'looked',\n",
       " 'This',\n",
       " 'I',\n",
       " '\"It',\n",
       " '\"you',\n",
       " '\"brush',\n",
       " '\"since',\n",
       " '\"I',\n",
       " '\"Stopping',\n",
       " '\"I',\n",
       " '\"You',\n",
       " '\"Jeez',\n",
       " '\"Anyone',\n",
       " '\"Locked',\n",
       " '\"When',\n",
       " 'Amazing',\n",
       " 'I',\n",
       " '\"is',\n",
       " 'Anybody',\n",
       " '\"wonders',\n",
       " '\"Even',\n",
       " 'I',\n",
       " '\"is',\n",
       " '\"Just',\n",
       " 'When',\n",
       " '\"You',\n",
       " '\"good',\n",
       " 'I',\n",
       " 'just',\n",
       " 'Sewing',\n",
       " '\"A',\n",
       " 'Good',\n",
       " 'I',\n",
       " '\"My',\n",
       " 'Why',\n",
       " \"Didn't\",\n",
       " '\"I',\n",
       " \"It's\",\n",
       " '\"Tea',\n",
       " '\"Well,',\n",
       " '\"My',\n",
       " '\"My',\n",
       " '\"So,',\n",
       " '\"Are',\n",
       " '\"True',\n",
       " '\"Just',\n",
       " '\"Enjoying',\n",
       " '\"Freezing',\n",
       " '\"Had',\n",
       " '\"Ohhh',\n",
       " '\"Passin',\n",
       " '\"Take',\n",
       " '\"is',\n",
       " '\"Dear',\n",
       " '\"has',\n",
       " '\"would',\n",
       " 'had',\n",
       " '\"loved',\n",
       " 'Wie',\n",
       " '\"Dear',\n",
       " \"Don't\",\n",
       " '\"lol',\n",
       " '\"The',\n",
       " 'you',\n",
       " '*almost',\n",
       " '\"/hahah!',\n",
       " '\"#',\n",
       " '\"For',\n",
       " '\"Grams:',\n",
       " '\"If',\n",
       " '\"Me:',\n",
       " '\"So',\n",
       " '\"SO!',\n",
       " 'The',\n",
       " 'Tony:',\n",
       " '\"When',\n",
       " '\"-/Are',\n",
       " '\"/Mitsubishis',\n",
       " '/All',\n",
       " 'Apparently',\n",
       " 'Just',\n",
       " '\"So',\n",
       " '\"Thanksgiving:',\n",
       " 'Having',\n",
       " '\"Because',\n",
       " 'bored',\n",
       " '\"Tom',\n",
       " 'The',\n",
       " '\"does',\n",
       " 'i',\n",
       " '\"I',\n",
       " 'whahahahahaa',\n",
       " '\"As',\n",
       " '\"has',\n",
       " 'too',\n",
       " 'excited',\n",
       " '\"new',\n",
       " '\"Okay',\n",
       " '\"you',\n",
       " '\"I',\n",
       " '\"quote',\n",
       " 'The',\n",
       " '\"Your',\n",
       " '\"Go',\n",
       " '\"Lord',\n",
       " '\"loves',\n",
       " '\"Quote',\n",
       " 'watched',\n",
       " 'But',\n",
       " '\"I',\n",
       " 'Is',\n",
       " 'Annoyed',\n",
       " 'BOTHER',\n",
       " '\"Clench',\n",
       " '\"I\\'m',\n",
       " '\"foods,',\n",
       " '\"If',\n",
       " '\"When',\n",
       " '\"I\\'m',\n",
       " '\"I\\'ve',\n",
       " '\"and',\n",
       " '\"MY',\n",
       " '\"weird',\n",
       " '\"and',\n",
       " '\"finishing',\n",
       " 'has',\n",
       " '\"I\\'m',\n",
       " 'is',\n",
       " '\"looking',\n",
       " '\"No',\n",
       " '\"Re-reading',\n",
       " 'SCOOOORE!!',\n",
       " '\"so',\n",
       " '\"WHOA...',\n",
       " 'Why',\n",
       " '\"YES!!!!!',\n",
       " '\"is',\n",
       " 'Who',\n",
       " '\"is',\n",
       " 'WE',\n",
       " '\"At',\n",
       " '\"my',\n",
       " '\"The',\n",
       " '\"Hay',\n",
       " '\"finally',\n",
       " '\"This',\n",
       " 'why',\n",
       " '\"This',\n",
       " 'i',\n",
       " '\"I\\'m',\n",
       " 'Maddie',\n",
       " 'Thus',\n",
       " '\"Trivium',\n",
       " '\"Okay',\n",
       " 'Happy',\n",
       " 'why',\n",
       " '\"I',\n",
       " '\"Fuck',\n",
       " '\"I',\n",
       " '\"I\\'m',\n",
       " '\"Cause',\n",
       " '\"Got',\n",
       " '\"Mmmm',\n",
       " 'I',\n",
       " '\"Gray',\n",
       " 'The',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"is',\n",
       " 'Just',\n",
       " '\"CSI:',\n",
       " 'Hero',\n",
       " 'I',\n",
       " '\"I\\'m',\n",
       " 'Sitting',\n",
       " '\"Lindsey',\n",
       " '\"If',\n",
       " '\"It\\'s',\n",
       " '\"oh,',\n",
       " '\"when',\n",
       " 'How',\n",
       " '\"Crackin\\'',\n",
       " '\"Have',\n",
       " 'You',\n",
       " 'Looks',\n",
       " '\"What',\n",
       " 'has',\n",
       " '\"Hours',\n",
       " '\"I\\'ve',\n",
       " 'Mrs.',\n",
       " '\"Off',\n",
       " '\"Time\\'s',\n",
       " '\"is',\n",
       " '\"is',\n",
       " '\"Dear',\n",
       " 'regrets',\n",
       " '\"You',\n",
       " '\"hated',\n",
       " '\"When',\n",
       " '\"act',\n",
       " '\"But',\n",
       " '\"Gotta',\n",
       " '\"Heigh',\n",
       " '\"I',\n",
       " '\"If',\n",
       " '\"There',\n",
       " '\"Thoughts',\n",
       " '\"You',\n",
       " '\"?\"\"Sometimes',\n",
       " '\"I',\n",
       " '\"I\\'m',\n",
       " '\"When',\n",
       " '\"asks',\n",
       " 'Woo!',\n",
       " '\"Wow!',\n",
       " '\"sometimes,',\n",
       " '\"??la',\n",
       " '\"Ma',\n",
       " '\":',\n",
       " '\":',\n",
       " ':',\n",
       " '\":',\n",
       " '\":',\n",
       " '\":',\n",
       " '\":',\n",
       " ':',\n",
       " '\":',\n",
       " '\":',\n",
       " '\":',\n",
       " 'Did',\n",
       " '\"Checkin\\'-out',\n",
       " '\"is',\n",
       " '\"Like,',\n",
       " '\"Augh,',\n",
       " '\"Auuuugh!',\n",
       " '\"Facebook',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"Sometimes',\n",
       " '\"Sorry',\n",
       " '\"We',\n",
       " 'Word',\n",
       " '\"A',\n",
       " '\"Good',\n",
       " '\"Iggy',\n",
       " 'This',\n",
       " 'i',\n",
       " 'My',\n",
       " 'Not',\n",
       " '\"Yes',\n",
       " 'Wow.',\n",
       " '*NOAA',\n",
       " '\"Cant',\n",
       " 'Why',\n",
       " '-.-/',\n",
       " '\"Dammit,',\n",
       " '\"I',\n",
       " '\"What',\n",
       " '\"*FACEDESKFACEDESKFACEDESK*',\n",
       " \"I'm\",\n",
       " 'Is',\n",
       " 'Is',\n",
       " '\"Jordan',\n",
       " '\"Still',\n",
       " 'Today',\n",
       " '\"***warning***',\n",
       " '\"Come',\n",
       " 'Happy',\n",
       " '\"How',\n",
       " '\"Is',\n",
       " 'Trying',\n",
       " '\"/Making',\n",
       " '\"Am',\n",
       " 'Erin',\n",
       " '\"Everyone',\n",
       " 'Hopefully',\n",
       " '\"I',\n",
       " 'If',\n",
       " 'It',\n",
       " 'Michael',\n",
       " '\"Off',\n",
       " '\"Thank',\n",
       " 'We',\n",
       " 'Why',\n",
       " 'Contemplating',\n",
       " '\"I',\n",
       " '\"I',\n",
       " '\"It',\n",
       " 'Laugh',\n",
       " '\"Life',\n",
       " 'My',\n",
       " '\"Working',\n",
       " '\"You\\'re',\n",
       " '\"At',\n",
       " 'Has',\n",
       " 'Is',\n",
       " '\"Looking',\n",
       " 'NO',\n",
       " '??',\n",
       " 'after',\n",
       " 'after',\n",
       " 'decided',\n",
       " '\"Dim',\n",
       " 'Has',\n",
       " '\"Has',\n",
       " '\"If',\n",
       " 'Is',\n",
       " 'Is',\n",
       " 'is',\n",
       " 'Is',\n",
       " '\"Is',\n",
       " 'Is',\n",
       " 'Ok',\n",
       " '\"penguin',\n",
       " 'Smally',\n",
       " '\"suspects',\n",
       " 'The',\n",
       " '\"Time',\n",
       " '\"today',\n",
       " '\"washing',\n",
       " \"I'm\",\n",
       " 'omgeez......sooo',\n",
       " '\"Golf',\n",
       " \"I'm\",\n",
       " '\"I\\'ve',\n",
       " '\"Just',\n",
       " 'Yet',\n",
       " '\"I',\n",
       " '\"Why',\n",
       " '\"You',\n",
       " \"Doesn't\",\n",
       " '\"Feels',\n",
       " '\"It',\n",
       " '\"Shh',\n",
       " \"There's\",\n",
       " '\"Apparently',\n",
       " '\"Hayden:',\n",
       " '\"Next',\n",
       " '\"The',\n",
       " '\"Two',\n",
       " 'You',\n",
       " 'I',\n",
       " '\"We',\n",
       " 'cannot',\n",
       " '\"kinda',\n",
       " \"Y'know\",\n",
       " '\"Haha,',\n",
       " 'Portugal',\n",
       " 'is',\n",
       " '\"Loves',\n",
       " 'Rainy',\n",
       " 'such',\n",
       " \"I'm\",\n",
       " '\"Love',\n",
       " 'I',\n",
       " '\"oh',\n",
       " '\";',\n",
       " '\";',\n",
       " '\";',\n",
       " '\"dearest',\n",
       " 'Old',\n",
       " '\"ATTENTION!!!',\n",
       " 'I',\n",
       " 'Some',\n",
       " '\"/Matthew\\'s',\n",
       " '\"I\\'m',\n",
       " 'mom',\n",
       " '\"/So',\n",
       " '\"HAVING',\n",
       " '\"MY',\n",
       " 'Grrr...I',\n",
       " '\"Facebook',\n",
       " '\"Thanks',\n",
       " '\"/Basically',\n",
       " '\"Dear',\n",
       " '\"Especially',\n",
       " '\"Apparently,',\n",
       " '\"Hoping',\n",
       " '\"is',\n",
       " '\"Leave',\n",
       " 'Somehow',\n",
       " '\"spent',\n",
       " '\"was',\n",
       " 'Who',\n",
       " 'How',\n",
       " '\"so',\n",
       " '\"saw',\n",
       " '\"And',\n",
       " 'Now',\n",
       " 'Has',\n",
       " '\"if',\n",
       " 'Wants',\n",
       " 'I',\n",
       " 'Almost',\n",
       " 'About',\n",
       " 'I',\n",
       " 'I',\n",
       " 'If',\n",
       " 'mother',\n",
       " 'why',\n",
       " '\"Hey',\n",
       " 'I',\n",
       " '\"is',\n",
       " 'twisting',\n",
       " 'bad',\n",
       " 'listening',\n",
       " 'recovering',\n",
       " '\"Come...',\n",
       " 'Status:',\n",
       " '\"/I',\n",
       " '\"Goddess',\n",
       " '\"You',\n",
       " \"It's\",\n",
       " '\"I\\'ve',\n",
       " '\"Hah...',\n",
       " '\"I',\n",
       " '\"My',\n",
       " '\"People',\n",
       " 'Crossing',\n",
       " '\"Nemo',\n",
       " '\"The',\n",
       " '\"?\"\"Work',\n",
       " 'Ah',\n",
       " 'Cuddling',\n",
       " '\"Finally',\n",
       " 'Oo',\n",
       " 'oo',\n",
       " 'So',\n",
       " 'So',\n",
       " '\"Spending']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data['text'], data['negative_yn_self'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/afs/inf.ed.ac.uk/user/s16/s1690903/share/fasttext/wikipedia.300d.txt')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(data['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
